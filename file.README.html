<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta name="Content-Type" content="text/html; charset=utf-8" />
<title>Arachni - Web Application Security Scanner Framework</title>
<link rel="stylesheet" href="css/style.css" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="css/common.css" type="text/css" media="screen" charset="utf-8" />

<script type="text/javascript" charset="utf-8">
  relpath = '';
  if (relpath != '') relpath += '/';
</script>
<script type="text/javascript" charset="utf-8" src="js/jquery.js"></script>
<script type="text/javascript" charset="utf-8" src="js/app.js"></script>

  </head>
  <body>
    <script type="text/javascript" charset="utf-8">
      if (window.top.frames.main) document.body.className = 'frames';
    </script>
    
    <div id="header">
      <div id="menu">
  
    <a href="_index.html" title="Index">Index</a> &raquo; 
    <span class="title">File: README</span>
  
  
  <div class="noframes"><span class="title">(</span><a href="." target="_top">no frames</a><span class="title">)</span></div>
</div>

      <div id="search">
  <a id="class_list_link" href="#">Class List</a>
  <a id="method_list_link" href="#">Method List</a>
  <a id ="file_list_link" href="#">File List</a>
</div>

      <div class="clear"></div>
    </div>
    
    <iframe id="search_frame"></iframe>
    
    <div id="content"><div id='filecontents'><h1>Arachni - Web Application Security Scanner Framework</h1>

<p><strong>Version</strong>:     0.2.1<br/>
<strong>Homepage</strong>:     <a href="http://github.com/zapotek/arachni">http://github.com/zapotek/arachni</a><br/>
<strong>News</strong>:     <a href="http://trainofthought.segfault.gr/category/projects/arachni/">http://trainofthought.segfault.gr/category/projects/arachni/</a><br/>
<strong>Documentation</strong>:     <a href="http://github.com/Zapotek/arachni/wiki">http://github.com/Zapotek/arachni/wiki</a><br/>
<strong>Code Documentation</strong>:     <a href="http://zapotek.github.com/arachni/">http://zapotek.github.com/arachni/</a><br/>
<strong>Google Group</strong>: <a href="http://groups.google.com/group/arachni">http://groups.google.com/group/arachni</a><br/>
<strong>Author</strong>:       <a href="mailto:tasos.laskos@gmail.com">Tasos</a> "<a href="mailto:zapotek@segfault.gr">Zapotek</a>" <a href="mailto:tasos.laskos@gmail.com">Laskos</a><br/>
<strong>Twitter</strong>:      <a href="http://twitter.com/Zap0tek">http://twitter.com/Zap0tek</a><br/>
<strong>Copyright</strong>:    2010<br/>
<strong>License</strong>:      <a href="file.LICENSE.html">GNU General Public License v2</a></p>

<p><img src="http://zapotek.github.com/arachni/logo.png" alt="Arachni logo" /></p>

<p>Kindly sponsored by: <a href="http://www.nopsec.com"><img src="http://zapotek.github.com/arachni/nopsec_logo.png" alt="NopSec" /></a></p>

<h2>Synopsis</h2>

<p>Arachni is a feature-full, modular, high-performance Ruby framework aimed towards helping
penetration testers and administrators evaluate the security of web applications.</p>

<p>Arachni is smart, it trains itself by learning from the HTTP responses it receives during the audit process.<br/>
Unlike other scanners, Arachni takes into account the dynamic nature of web applications and can detect changes caused while travelling<br/>
through the paths of a web application's cyclomatic complexity.<br/>
This way attack/input vectors that would otherwise be undetectable by non-humans are seamlessly handled by Arachni.</p>

<p>Finally, Arachni yields great performance due to its asynchronous HTTP  model (courtesy of <a href="https://github.com/pauldix/typhoeus">Typhoeus</a>).<br/>
Thus, you'll only be limited by the responsiveness of the server under audit and your available bandwidth.</p>

<p><strong>Note</strong>: <em>Despite the fact that Arachni is mostly targeted towards web application security, it can easily be used for general purpose scraping, data-mining, etc with the addition of custom modules.</em></p>

<h3>Arachni offers:</h3>

<h4>A stable, efficient, high-performance framework</h4>

<p>Module, report and plugin writers are allowed to easily and quickly create and deploy their components
with the minimum amount of restrictions imposed upon them, while provided with the necessary infrastructure to accomplish their goals.<br/>
Furthermore, they are encouraged to take full advantage of the Ruby language under a unified framework that will increase their productivity
without stifling them or complicating their tasks.<br/></p>

<h4>Simplicity</h4>

<p>Although some parts of the Framework are fairly complex you will never have to deal them directly.<br/>
From a user's or a component developer's point of view everything appears simple and straight-forward all the while providing power, performance and flexibility.</p>

<h2>Feature List</h2>

<h3>General</h3>

<ul>
<li>Cookie-jar support</li>
<li>SSL support.</li>
<li>User Agent spoofing.</li>
<li>Proxy support for SOCKS4, SOCKS4A, SOCKS5, HTTP/1.1 and HTTP/1.0.</li>
<li>Proxy authentication.</li>
<li>Site authentication (Automated form-based, Cookie-Jar, Basic-Digest, NTLM and others)</li>
<li>Highlighted command line output.</li>
<li>UI abstraction.

<ul>
<li>Command line UI</li>
<li>XMLRPC command line client/server</li>
</ul>
</li>
<li>Pause/resume functionality.</li>
<li>High performance asynchronous HTTP requests.</li>
</ul>


<h3>Website Crawler</h3>

<p>The crawler is provided by a modified version of <a href="http://anemone.rubyforge.org/">Anemone</a>.</p>

<ul>
<li>Filters for redundant pages like galleries, catalogs, etc based on regular expressions and counters.</li>
<li>URL exclusion filter based on regular expressions.</li>
<li>URL inclusion filter based on regular expressions.</li>
<li>Can optionally follow subdomains.</li>
<li>Adjustable depth limit.</li>
<li>Adjustable link count limit.</li>
<li>Adjustable redirect limit.</li>
<li>Modular path extraction via "Path Extractor" components.</li>
</ul>


<h3>HTML Parser</h3>

<p>Can extract and analyze:</p>

<ul>
<li>Forms</li>
<li>Links</li>
<li>Cookies</li>
</ul>


<p>The analyzer can graciously handle badly written HTML code due to a combination of regular expression analysis and the <a href="http://nokogiri.org/">Nokogiri</a> HTML parser.</p>

<h3>Module Management</h3>

<ul>
<li>Very simple and easy to use module API providing access to multiple levels of complexity.</li>
<li>Helper audit methods:

<ul>
<li>For forms, links and cookies auditing.</li>
<li>A wide range of injection strings/input combinations.</li>
<li>Writing RFI, SQL injection, XSS etc modules is a matter of minutes if not seconds.</li>
</ul>
</li>
<li>Currently available modules:

<ul>
<li>Audit:

<ul>
<li>Blind SQL injection</li>
<li>CSRF detection</li>
<li>Eval/Code injection</li>
<li>LDAP injection</li>
<li>Path traversal</li>
<li>Response splitting</li>
<li>OS command injection</li>
<li>Remote file inclusion</li>
<li>SQL injection</li>
<li>Unvalidated redirects</li>
<li>XPath injection</li>
<li>Path XSS</li>
<li>URI XSS</li>
<li>XSS</li>
</ul>
</li>
<li>Recon:

<ul>
<li>Allowed HTTP methods</li>
<li>Back-up files</li>
<li>Common directories</li>
<li>Common files</li>
<li>HTTP PUT</li>
<li>Insufficient Transport Layer Protection for password forms</li>
<li>WebDAV detection</li>
<li>HTTP TRACE detection</li>
<li>Credit Card number disclosure</li>
<li>CVS/SVN user disclosure</li>
<li>Private IP address disclosure</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>Report Management</h3>

<ul>
<li>Modular design.</li>
<li>Currently available reports:

<ul>
<li>Standard output</li>
<li>HTML</li>
<li>XML</li>
<li>TXT</li>
<li>YAML serialization</li>
<li>Metareport (providing Metasploit integration to allow for <a href="http://zapotek.github.com/arachni/file.EXPLOITATION.html">automated and assisted exploitation</a>)</li>
</ul>
</li>
</ul>


<h3>Plug-in Management</h3>

<ul>
<li>Modular design</li>
<li>Plug-ins are framework demi-gods, they have direct access to the framework instance.</li>
<li>Can be used to add any functionality to Arachni.</li>
<li>Currently available plugins:

<ul>
<li>Passive Proxy</li>
<li>Form based AutoLogin</li>
</ul>
</li>
</ul>


<h3>Trainer subsystem</h3>

<p>The Trainer is what enables Arachni to learn from the scan it performs and incorporate that knowledge, on the fly, for the duration of the audit.</p>

<p>Modules have the ability to individually force the Framework to learn from the HTTP responses they are going to induce.<br/>
However, this is usually not required since Arachni is aware of which requests are more likely to uncover new elements or attack vectors and will adapt itself accordingly.</p>

<p>Still, this can be an invaluable asset to Fuzzer modules.</p>

<h2>Usage</h2>

<pre class="code">   Arachni - Web Application Security Scanner Framework v0.2.1 [0.2]
   Author: Tasos &quot;Zapotek&quot; Laskos &lt;tasos.laskos@gmail.com&gt;
                                  &lt;zapotek@segfault.gr&gt;
           (With the support of the community and the Arachni Team.)

   Website:       http://github.com/Zapotek/arachni
   Documentation: http://github.com/Zapotek/arachni/wiki


  Usage:  arachni [options] url

  Supported options:
</pre>

<h3>General</h3>

<pre class="code">-h
--help                      output this

-v                          be verbose

--debug                     show what is happening internally
                              (You should give it a shot sometime ;) )

--only-positives            echo positive results *only*

--http-req-limit            concurent HTTP requests limit
                              (Be carefull not to kill your server.)
                              (Default: 200)
                              (NOTE: If your scan seems unresponsive try lowering the limit.)

--http-harvest-last         build up the HTTP request queue of the audit for the whole site
                             and harvest the HTTP responses at the end of the crawl.
                             (Default: responses will be harvested for each page)
                             (*NOTE*: If you are scanning a high-end server and
                               you are using a powerful machine with enough bandwidth
                               *and* you feel dangerous you can use
                               this flag with an increased '--http-req-limit'
                               to get maximum performance out of your scan.)
                             (*WARNING*: When scanning large websites with hundreads
                              of pages this could eat up all your memory pretty quickly.)

--cookie-jar=&lt;cookiejar&gt;    netscape HTTP cookie file, use curl to create it


--user-agent=&lt;user agent&gt;   specify user agent

--authed-by=&lt;who&gt;           who authorized the scan, include name and e-mail address
                              (It'll make it easier on the sys-admins during log reviews.)
                              (Will be appended to the user-agent string.)
</pre>

<h3>Profiles</h3>

<pre class="code">--save-profile=&lt;file&gt;       save the current run profile/options to &lt;file&gt;
                              (The file will be saved with an extention of: .afp)

--load-profile=&lt;file&gt;       load a run profile from &lt;file&gt;
                              (Can be used multiple times.)
                              (You can complement it with more options, except for:
                                  * --mods
                                  * --redundant)

--show-profile              will output the running profile as CLI arguments
</pre>

<h3>Crawler</h3>

<pre class="code">-e &lt;regex&gt;
--exclude=&lt;regex&gt;           exclude urls matching regex
                              (Can be used multiple times.)

-i &lt;regex&gt;
--include=&lt;regex&gt;           include urls matching this regex only
                              (Can be used multiple times.)

--redundant=&lt;regex&gt;:&lt;count&gt; limit crawl on redundant pages like galleries or catalogs
                              (URLs matching &lt;regex&gt; will be crawled &lt;count&gt; links deep.)
                              (Can be used multiple times.)

-f
--follow-subdomains         follow links to subdomains (default: off)

--obey-robots-txt           obey robots.txt file (default: off)

--depth=&lt;number&gt;            depth limit (default: inf)
                              (How deep Arachni should go into the site structure.)

--link-count=&lt;number&gt;       how many links to follow (default: inf)

--redirect-limit=&lt;number&gt;   how many redirects to follow (default: inf)
</pre>

<h3>Auditor</h3>

<pre class="code">-g
--audit-links               audit link variables (GET)

-p
--audit-forms               audit form variables
                              (usually POST, can also be GET)

-c
--audit-cookies             audit cookies (COOKIE)

--exclude-cookie=&lt;name&gt;     cookies not to audit
                              (You should exclude session cookies.)
                              (Can be used multiple times.)

--audit-headers             audit HTTP headers
                              (*NOTE*: Header audits use brute force.
                               Almost all valid HTTP request headers will be audited
                               even if there's no indication that the web app uses them.)
                              (*WARNING*: Enabling this option will result in increased requests,
                               maybe by an order of magnitude.)
</pre>

<h3>Modules</h3>

<pre class="code">--lsmod=&lt;regexp&gt;            list available modules based on the provided regular expression
                              (If no regexp is provided all modules will be listed.)
                              (Can be used multiple times.)


-m &lt;modname,modname..&gt;
--mods=&lt;modname,modname..&gt;  comma separated list of modules to deploy
                              (Use '*' to deploy all modules)
                              (You can exclude modules by prefixing their name with a dash:
                                  --mods=*,-backup_files,-xss
                               The above will load all modules except for the 'backup_files' and 'xss' modules. )
</pre>

<h3>Reports</h3>

<pre class="code">--lsrep                       list available reports

--repload=&lt;file&gt;              load audit results from an .afr file
                                (Allows you to create new reports from finished scans.)

--report='&lt;report&gt;:&lt;optname&gt;=&lt;val&gt;,&lt;optname2&gt;=&lt;val2&gt;,...'

                              &lt;report&gt;: the name of the report as displayed by '--lsrep'
                                (Default: stdout)
                                (Can be used multiple times.)
</pre>

<h3>Plugins</h3>

<pre class="code">--lsplug                      list available plugins

--plugin='&lt;plugin&gt;:&lt;optname&gt;=&lt;val&gt;,&lt;optname2&gt;=&lt;val2&gt;,...'

                              &lt;plugin&gt;: the name of the plugin as displayed by '--lsplug'
                                (Can be used multiple times.)
</pre>

<h3>Proxy</h3>

<pre class="code">--proxy=&lt;server:port&gt;       specify proxy

--proxy-auth=&lt;user:passwd&gt;  specify proxy auth credentials

--proxy-type=&lt;type&gt;           proxy type can be http, http_1_0, socks4, socks5, socks4a
                              (Default: http)
</pre>

<h3>Examples</h3>

<p>As of v0.2.1 you can simply run Arachni like so:</p>

<pre class="code">$ ./arachni.rb http://test.com
</pre>

<p>which will load all modules and audit all forms, links and cookies.</p>

<p>In the following example all modules will be run against <i>http://test.com</i>, auditing links/forms/cookies and following subdomains --with verbose output enabled.<br/>
The results of the audit will be saved in the the file <i>test.com.afr</i>.</p>

<pre class="code">$ ./arachni.rb -fv http://test.com --report=afr:outfile=test.com.afr
</pre>

<p>The Arachni Framework Report (.afr) file can later be loaded by Arachni to create a report, like so:</p>

<pre class="code">$ ./arachni.rb --repload=test.com.afr --report=html:outfile=my_report.html
</pre>

<p>or any other report type as shown by:</p>

<pre class="code">$ ./arachni.rb --lsrep
</pre>

<p>For a full explanation of all available options you can consult the <a href="http://github.com/Zapotek/arachni/wiki/User-guide">User Guide</a>.</p>

<h2>Requirements</h2>

<p>As of version 0.2.1 Arachni will also be released as <a href="http://stanford.edu/~pgbovine/cde.html">CDE packages</a> for 32bit and 64bit architectures.<br/>
CDE packages are self contained and thus alleviate the need for Ruby and other dependencies to be installed.<br/>
You can choose the CDE package that suits you best from the <a href="https://github.com/Zapotek/arachni/downloads">download</a> page and escape the dependency hell.<br/></p>

<p>Otherwise, in order to use Arachni you will need the following:</p>

<ul>
<li>ruby1.9.1 or later (<strong>Version 1.9.2 would be preferable.</strong>)</li>
<li>Nokogiri</li>
<li>Typhoeus</li>
<li>Awesome print</li>
<li>Liquid (for HTML reporting)</li>
<li>Yardoc (to generate the documentation)</li>
<li>Robots</li>
</ul>


<p>Run the following to install all required system libraries:</p>

<pre class="code">sudo apt-get install libxml2-dev libxslt1-dev libcurl4-openssl-dev ruby1.9.1-full ruby1.8-dev rubygems
</pre>

<p><em>Adapt the above line to your Linux distro.</em></p>

<p>Users are <strong>strongly</strong> encouraged to compile Ruby 1.9.2 from source and use it to run Arachni,
in which case the following system packages don't need to be installed:</p>

<pre class="code">ruby1.9.1-full ruby1.8-dev rubygems
</pre>

<p>Run the following to install all gem dependencies:</p>

<pre class="code"><span class='id sudo'>sudo</span> <span class='id gem'>gem</span> <span class='id install'>install</span> <span class='id nokogiri'>nokogiri</span> <span class='id typhoeus'>typhoeus</span> <span class='id awesome_print'>awesome_print</span> <span class='id liquid'>liquid</span> <span class='id yard'>yard</span> <span class='id robots'>robots</span>
</pre>

<p><em>If you have more than one Ruby version installed make sure that you install the gems and run Arachni with the proper version.</em></p>

<h2>Supported platforms</h2>

<p>Arachni should work on all *nix and POSIX compliant platforms with Ruby
and the aforementioned requirements.</p>

<p>Windows users should run Arachni in Cygwin.</p>

<h2>Bug reports/Feature requests</h2>

<p>Please send your feedback using Github's issue system at
<a href="http://github.com/zapotek/arachni/issues">http://github.com/zapotek/arachni/issues</a>.</p>

<h2>License</h2>

<p>Arachni is licensed under the GNU General Public License v2.<br/>
See the <a href="file.LICENSE.html">LICENSE</a> file for more information.</p>

<h2>Disclaimer</h2>

<p>Arachni is free software and you are allowed to use it as you see fit.<br/>
However, I can't be held responsible for your actions or for any damage
caused by the use of this software.</p>

<p><img src="http://zapotek.github.com/arachni/banner.png" alt="Arachni banner" /></p></div></div>
    
    <div id="footer">
  Generated on Sun Dec  5 16:37:39 2010 by 
  <a href="http://yardoc.org" title="Yay! A Ruby Documentation Tool" target="_parent">yard</a>
  0.6.2 (ruby-1.9.2).
</div>

  </body>
</html>